##### 数据从哪里来, 数据到哪里去 技术架构

##### 1.数据采集
    1.1 数据从无到有的过程(服务器打印的log,自定义采集的日志等)
    1.2 另一方面也有把通过使用Flume等工具把数据采集搬运到指定位置的这个过程叫数据采集

##### 2.数据预处理
          数据预处理,是指在正式处理以前对数据进行一些处理,保证后续正式处理的数据是格式统一
          干净规整的结构化数据
         
          现实中数据大体上都是不完整,不一致的脏数据,无法直接进行数据分析
         
          数据预处理的几种方法:
         
          < 1.数据清理
          < 2.数据集成
          < 3.数据变换
         
          技术选型: 任何语言软件只要能够接收数据,处理数据,并且最终输出数据,都可以用于数据处理
         
          选择 mapReduce  mr是Java程序
         
          mr是一个分布式程序,如果数据量大可以分布式并行计算处理,提高效率
          
##### 3.数据入库(ETL)

      库:面向分析的数据仓库,也就是 apache hive
     
      入库本质:经过ETL(抽取,转化,加载) 把各个不同数据源集中加载到数据仓库的分析主题下
     
      ETL 本质,经过我们的抽取转换加载,把各个不同的数据源数据加载至我们数据仓库中的过程
      这个过程通俗的说法叫做数据的入库,专业说法ETL
     
      预处理完的结构化数据通常会导入到Hive数据仓库中,建立的库和表与之映射关联
     
      这样后续就可以使用 Hive Sql针对数据进行分析
     
      因此 这里所说的入库,就是把数据加进面向分析的数据仓库
     
      实际中入库过程有个更加专业的叫法 ETL ETL是将业务系统的数据经抽取
      清洗转换之后加载到数据库的过程,目的是将企业中的分散,凌乱标准不统一
      的数据整合到一起,为企业的决策提供分析依据
     
      ETL的设计分三部分 数据抽取 数据的清洗转换 数据的加载

##### 4.数据分析
    本阶段是项目核心内容,即根据需求使用 Hive Sql分析语句,得出指标各种统计结果

##### 5.数据可视化
    将分析所得数据结构结果进行可视化,一般通过图表展示